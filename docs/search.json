[
  {
    "objectID": "gradbasics.html",
    "href": "gradbasics.html",
    "title": "Gradients Bootcamp",
    "section": "",
    "text": "This notebook was creating while following Andrej Karparthy’s YouTube content (thank you).\nRecreation of the ‘micrograd’ library.\nIt creates a generalized, simple framework for composing compound functions, basic neural networks, manage forward and backwards passes, gradient decent, and model training."
  },
  {
    "objectID": "gradbasics.html#basic-quadratic-example",
    "href": "gradbasics.html#basic-quadratic-example",
    "title": "Gradients Bootcamp",
    "section": "Basic Quadratic Example",
    "text": "Basic Quadratic Example\nIn this first example, we explore a basic quadratic function and empirically calculate derivatives.\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# basic quadratic\ndef f(x):\n    return 3*x**2 - 4*x + 5\nf(3.0)\n\n20.0\n\n\n\n# plotting\nxs = np.arange(-5, 5, 0.25)\nys = f(xs)\nplt.plot(xs, ys)\n\n\n\n\n\n\n\n\n\n# empirical derivative calculation at specific point\nh = 0.0000000001\nx = 3.0\n(f(x+h)-f(x))/h\n\n14.000001158365194\n\n\n\n# partial derivative example\nh = 0.000001\na = 2.0\nb = -3.0\nc = 10.0\n\nd1 = a*b + c\na += h\nd2 = a*b + c\n\nprint('d1', d1)\nprint('d2', d2)\nprint('slope', (d2-d1)/h)\n\nd1 4.0\nd2 3.9999969999999996\nslope -3.000000000419334"
  },
  {
    "objectID": "gradbasics.html#building-the-value-object",
    "href": "gradbasics.html#building-the-value-object",
    "title": "Gradients Bootcamp",
    "section": "Building The “Value” Object",
    "text": "Building The “Value” Object\nCreating a class to represent each ‘node’ or element in a function (or network)\n\nThe value class will serve as the primary object for nodes in the network\n\nA node is an object defined by a value and relationship to other nodes\n\nEach operation must have a clearly defined way to calculate and back-propogate\n\nEach node needs knowledge of it’s ‘creating’ nodes and the operation used to create\n\n\n# defining basic value object and operations for forward and backward prop\n\nclass Value:\n# the value class will serve as the primary object for nodes in the network\n# a node is an object defined by a value and relationship to other nodes\n# each operation must have a clearly defined way to calculate and back-propogate\n# each node needs knowledge of it's 'creating' nodes and the operation used to create\n    \n    def __init__(self, data, _children=(), _op='', label=''):\n        # define data value; as well as gradient, a 'back-prop' function, child nodes, operation, and label\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        # print 'meaningful' representation of object when using notebook\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        # define addition and return reference Value objects and operator\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        # define 'backward pass' gradient definitions based on chain rule\n        ## note +=  ...this accounts for multiple uses of a node in a network \n        ## we want to avoid 'overwriting' the gradient if two nodes back propogate to the same child\n        ## chain rule with multivariate data allows for the addition of gradients in this case\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n        \n    def __sub__(self, other):\n        return self + (-other)\n\n    def __mul__(self, other):\n        # define multiply and return reference Value objects and operator\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        # define gradients based on chain rule\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data**other, (self, ), f'**{other}')\n        def _backward():\n            self.grad += other * self.data**(other-1.0) * out.grad\n        out._backward = _backward\n        return out\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n    def __rmul__(self, other): # other * self\n        return self * other\n\n    def __radd__(self, other):\n        return self + other\n\n    def exp(self):\n        x = self.data\n        out = Value(math.exp(x), (self, ), 'exp')\n\n        def _backward():\n            # e**x is local derivative of e**x\n            self.grad += out.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def tanh(self):\n        # define tanh (e**2x - 1) / (e**2x + 1)  === -1 to 1 range squish\n        x = self.data\n        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n        out = Value(t, (self, ), 'tanh')\n        def _backward():\n            # looked this shit up - derivative of tanh\n            # could be derived with basic calculus -- will do later\n            self.grad += (1 - t**2) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        # we first build a topography - an ordered list of the parent &gt; child chains\n        topo=[]\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    # recursive through the children\n                    build_topo(child)\n                # at bottom of trace, start appending\n                topo.append(v)\n        build_topo(self)\n\n        # set initial gradient of final output node L:  dL/dL (always 1)\n        self.grad = 1.0\n        for node in reversed(topo):\n            # execute the 'backward' functions for each node to calculate gradients from top to bottom\n            node._backward()\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nd = a*b; d.label='d'\nc = d + Value(5.9, label='e'); c.label ='c'\nL = c.tanh(); L.label = 'L'\n\n\nc._prev\n\n{Value(data=-6.0), Value(data=5.9)}\n\n\n\nc._op\n\n'+'\n\n\n\n# visualization using graphviz api for node inputs\n\npathadd = \"C:/Program Files/Graphviz/bin/\"\nimport os\nos.environ[\"PATH\"] += os.pathsep + pathadd\nfrom graphviz import Digraph\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root):\n    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n    nodes, edges = trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad, ), shape='record')\n        if n._op:\n            dot.node(name=uid + n._op, label = n._op)\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot\n\n\ndraw_dot(L)\n\n\n\n\n\n\n\n\n\nL.backward()\n\n\ndraw_dot(L)\n\n\n\n\n\n\n\n\n\nx1 = Value(2.0, label='x1')\nx2 = Value(0.0, label='x2')\nw1 = Value(-3.0, label='w1')\nw2 = Value(1.0, label='w2')\nb = Value(6.8813735870195432, label='b')\nx1w1 = x1*w1; x1w1.label = 'x1*w1'\nx2w2 = x2*w2; x2w2.label = 'x2*w2'\nx1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\nn = x1w1x2w2 + b; n.label = 'n'\no = ((2*n).exp()-1)/((2*n).exp()+1); o.label = 'o'\no.backward()\n\n\ndraw_dot(o)"
  },
  {
    "objectID": "gradbasics.html#same-example-now-pytorch",
    "href": "gradbasics.html#same-example-now-pytorch",
    "title": "Gradients Bootcamp",
    "section": "Same example, now PyTorch",
    "text": "Same example, now PyTorch\n\n# pytorch representation\nimport torch\n\n\nx1 = torch.Tensor([2.0]).double(); x1.requires_grad=True\nx2 = torch.Tensor([0.0]).double(); x2.requires_grad=True\nw1 = torch.Tensor([-3.0]).double(); w1.requires_grad=True\nw2 = torch.Tensor([1.0]).double(); w2.requires_grad=True\nb = torch.Tensor([6.8813765870195432]).double(); b.requires_grad=True\n# above set to 'double' to match the 2 pointer float method of python / float 64 vs 32\nn = x1*w1 + x2*w2 + b\no = torch.tanh(n)\nprint(o.data.item())\no.backward()\nprint('-----')\nprint('x2', x2.grad.item())\nprint('w2', w2.grad.item())\nprint('x1', x1.grad.item())\nprint('w1', w1.grad.item())\n\n0.707108359331579\n-----\nx2 0.4999977681634026\nw2 0.0\nx1 -1.4999933044902076\nw1 0.9999955363268052"
  },
  {
    "objectID": "gradbasics.html#basic-neural-network-from-our-base-value-class",
    "href": "gradbasics.html#basic-neural-network-from-our-base-value-class",
    "title": "Gradients Bootcamp",
    "section": "Basic Neural Network from our base Value class",
    "text": "Basic Neural Network from our base Value class\nCreating Neuron definition based on perceptron nerve model: wx + b\n\n# basic neural network - 2 layer perceptron\n\nimport random\n\nclass Neuron:\n\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n        self.b = Value(random.uniform(-1,1))\n\n    def __call__(self, x):\n        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n        out = act.tanh()\n        return out\n\n    def parameters(self):\n        return self.w + [self.b]\n\nclass Layer:\n\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.parameters()]\n\nx = [2.0, 3.0]\nn = Neuron(2)\nn(x)\n\nValue(data=0.7710059294619084)\n\n\n\nx = [2.0, 3.0]\nn = Layer(2, 3)\nn(x)\n\n[Value(data=-0.9955879131700199),\n Value(data=-0.5097029591322493),\n Value(data=0.9965281438417336)]\n\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\nx = [2.0, 3.0, -1.0]\nn = MLP(3, [4, 4, 1])\nn(x)\n\nValue(data=-0.7692074529154024)\n\n\n\ny = [2.0, 3.0]\nn2 = MLP(2, [3, 3, 1])\nn2(x)\n\nValue(data=-0.7019997780354041)\n\n\n\ndraw_dot(n2(x))\n\n\n\n\n\n\n\n\n\nn = MLP(3, [3, 3, 1])\n\nxs = [\n[2.0, 3.0, -1.0],\n[3.0, -1.0, 0.5],\n[0.5, 1.0, 1.0],\n[1.0, 1.0, -1.0],\n]\nys = [1.0, -1.0, -1.0, 1.0] # targets\n\n\n# training loop for network\n\nfor k in range(20):\n    \n    # forward pass\n    ypred = [n(x) for x in xs]\n    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n    \n    # backwards pass\n    for p in n.parameters():\n        p.grad = 0.0\n    loss.backward()\n\n    # update\n    for p in n.parameters():\n        p.data += -0.1 * p.grad\n    \n\n    print(k, loss.data)\n\n0 6.716117378416472\n1 5.556697957977596\n2 3.649244984903412\n3 3.15756261040604\n4 2.922059534006541\n5 2.783226211789527\n6 2.6630413770238532\n7 2.510006135006954\n8 2.240607886127641\n9 1.7776929442612133\n10 1.2132704181393033\n11 0.7505110676829165\n12 0.4528071853316977\n13 0.2940499244954832\n14 0.21003762632407547\n15 0.1648694901876382\n16 0.13508731573001465\n17 0.11411403053452894\n18 0.09860667977214621\n19 0.08670487408481826\n\n\n\nypred\n\n[Value(data=0.8766735146392082),\n Value(data=-0.8011294912314902),\n Value(data=-0.9318566380835971),\n Value(data=0.8347654543920767)]\n\n\n\ndraw_dot(ypred[0])"
  }
]